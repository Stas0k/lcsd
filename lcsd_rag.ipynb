{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color: #4daafc\">Legal Case Similarity Detection - RAG</span>\n",
    "\n",
    "- [Environment](#environment)\n",
    "- [Load data](#load-data)\n",
    "- [Hybrid search](#hybrid-search)\n",
    "- [RAG QA Chat](#rag-qa-chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.embedding import get_embedding_handler\n",
    "from utils.db import load_vector_db\n",
    "from utils.str_utils import str_to_arr\n",
    "from langchain.schema import Document\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from langchain_core.tools import tool\n",
    "from typing import Dict\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from langchain_aws.chat_models import ChatBedrock\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.vectorstores.base import VectorStoreRetriever\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore specific warning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://localhost:11434\"\n",
    "emb_model = 'nomic-embed-text:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dense vector DB (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_vector_db: successfully loaded vector db from db/vectors/db_legal_cases_summary_100\n"
     ]
    }
   ],
   "source": [
    "db_name = \"db/vectors/db_legal_cases_summary_100\"\n",
    "embedding_handler = get_embedding_handler(model=emb_model, base_url=base_url)\n",
    "vector_store = load_vector_db(db_name, embedding_handler, trust_source=True)\n",
    "retriever = vector_store.as_retriever(search_type = 'similarity', search_kwargs = {'k': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4a78a8dc-6462-4bf8-8bfb-dd303f955d54', 0),\n",
       " ('26c2c7c0-d2ec-4904-96af-5bea22997835', 1),\n",
       " ('3feb2b4a-9232-4093-ad77-bd973f435deb', 2),\n",
       " ('50b59d56-2945-40cf-b790-d149a1ea9a2b', 3),\n",
       " ('73b31cce-2d1a-4c75-be3f-abfc16eeaf35', 4),\n",
       " ('86e65359-d7ba-4282-a506-50603d6ba493', 5),\n",
       " ('51bc9427-8ac8-410f-ac18-5d50930f1b4d', 6),\n",
       " ('122faeac-657c-4509-bf9d-169547f97a7a', 7),\n",
       " ('6ad5fa41-3001-4028-8d6e-b497580b1e60', 8),\n",
       " ('70b7159b-8366-4745-8ffe-807326a5dc58', 9)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docstore_id_to_indx = {value: key for key, value in vector_store.index_to_docstore_id.items()}\n",
    "list(docstore_id_to_indx.items())[:10] # display first n elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sparse vectors (One-hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path_sparse_vec = \"db/vectors/sparse_vectors.npy\"\n",
    "sparse_vectors = np.load(f_path_sparse_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hybrid search tool function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def hybrid_search(doc_id: str, alpha: float = 0.5, k: int = 5, sparse_exp: float = 0.5) -> Dict[str, float] | str:\n",
    "    \"\"\"\n",
    "    Find similar documents/legal cases in the database. \n",
    "\n",
    "    Args:\n",
    "        doc_id (str): document identifier (legal case number). Example: 3015/09\n",
    "        alpha (float): Weighting factor for dense vs sparse search (0.0 = only sparse, 1.0 = only dense).\n",
    "        k (int): Number of top results to return.\n",
    "        sparse_exp (float): Power exponent to apply to sparse scores to boost their impact.\n",
    "            Values < 1 (e.g., 0.5) boost small similarity scores; values > 1 reduce them.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Mapping of case_number to hybrid similarity score.\n",
    "    \"\"\"\n",
    "    if not doc_id:\n",
    "        return \"Error: 'doc_id' is required as input.\"\n",
    "\n",
    "    # check if the document ID exists in the database\n",
    "    doc: Document = next((d for d in vector_store.docstore._dict.values() if d.metadata.get(\"case_number\") == doc_id), None)\n",
    "    if not doc:\n",
    "        return f\"Document ID '{doc_id}' not found in the database.\"\n",
    "    \n",
    "    # get the index of the document we're querying\n",
    "    query_doc_idx = docstore_id_to_indx.get(doc.id)\n",
    "    \n",
    "    # get the embedding for the user's document. Normalized dense vector (L2 norm = 1)\n",
    "    query_dense_vector = embedding_handler.embed_documents([doc.page_content])[0]\n",
    "\n",
    "    # sparse vector (one-hot encoded)\n",
    "    query_sparse_vector = str_to_arr(doc.metadata['legal_refs_sparse_vec'])\n",
    "\n",
    "    # dense search (FAISS)\n",
    "    dense_results = vector_store.similarity_search_with_score_by_vector(query_dense_vector, k=k+1)\n",
    "\n",
    "    # extract FAISS document indexes and similarity scores\n",
    "    dense_scores = {docstore_id_to_indx.get(d.id): score for d, score in dense_results if docstore_id_to_indx.get(d.id) != query_doc_idx}\n",
    "\n",
    "    # sparse search using Jaccard similarity\n",
    "    # .reshape(1, -1) converts query_sparse_vector from shape (n,) (1D) to shape (1, n) (2D)\n",
    "    jaccard_distances = pairwise_distances(query_sparse_vector.reshape(1, -1), sparse_vectors, metric=\"jaccard\")[0]\n",
    "\n",
    "    # convert distances to similarity (Jaccard similarity = 1 - Jaccard distance)\n",
    "    sparse_scores = 1 - jaccard_distances\n",
    "\n",
    "    # since sparse scores have relatively small values comparing to dense vector cosine similarity scores, \n",
    "    # decided to scale-up the values using power exponent\n",
    "    sparse_scores = np.power(sparse_scores, sparse_exp)\n",
    "\n",
    "    # merge scores using weighted sum\n",
    "    combined_scores = {}\n",
    "    for idx in range(sparse_vectors.shape[0]):  # iterate through document indices\n",
    "        if idx != query_doc_idx:\n",
    "            dense_score = dense_scores.get(idx, 0)  # FAISS similarity score\n",
    "            sparse_score = sparse_scores[idx]  # Jaccard similarity score\n",
    "            combined_scores[idx] = alpha * dense_score + (1 - alpha) * sparse_score\n",
    "            # ucomment below line for debug\n",
    "            #print(f\"indx={idx}, dense score={float(dense_score)}, sparse_score={float(sparse_score)}, combined score={combined_scores[idx]}\")\n",
    "\n",
    "    # rank documents by combined score\n",
    "    ranked_docs = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # return top k documents\n",
    "    res = {vector_store.docstore._dict[vector_store.index_to_docstore_id[d_idx]].metadata['case_number']: float(d_scr) for d_idx, d_scr in ranked_docs[:k]}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG QA Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init langchain AWS Bedrock chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load environment variables from the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define params - model id, region, keys, etc.\n",
    "region_name = 'us-west-2'\n",
    "model_id = 'anthropic.claude-3-5-sonnet-20241022-v2:0'\n",
    "endpoint_url = 'https://bedrock-runtime.us-west-2.amazonaws.com'\n",
    "\n",
    "# get secret keys from environment variables\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "# timeout configuration\n",
    "timeout_config = Config(connect_timeout=30, read_timeout=120)\n",
    "\n",
    "# initialize Bedrock client\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name=region_name, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, endpoint_url=endpoint_url, config=timeout_config)\n",
    "\n",
    "# initialize the LangChain LLM Chat Bedrock\n",
    "llm = ChatBedrock(\n",
    "    client=bedrock_client,\n",
    "    model_id=model_id,\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0.5\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heler function that formats the documents content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define RAG question answering tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def legal_rag_qa(query: str) -> str:\n",
    "    \"\"\"Answer general legal questions using retrieved documents.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query.\n",
    "    \"\"\"\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    context = format_docs(docs)\n",
    "    #print(f\"Context ===>> {context}\")\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a legal assistant. Use the provided context to answer the user's query.\\nContext:\\n{context}\"),\n",
    "        (\"human\", \"Query: {query}\")\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llm\n",
    "    return chain.invoke({\"context\": context, \"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_947517/3353824168.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a legal assistant that uses tools to retrieve similar cases or answer legal questions.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name='agent_scratchpad')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [hybrid_search, legal_rag_qa]\n",
    "agent_runnable = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentExecutor(\n",
    "    agent=agent_runnable,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=False  # put True for debug\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results debug function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(result):\n",
    "    output = result.get(\"output\", [])\n",
    "\n",
    "    if isinstance(output, list):\n",
    "        for block in output:\n",
    "            if block.get(\"type\") == \"text\":\n",
    "                print(\"LLM output:\", block[\"text\"])\n",
    "    elif isinstance(output, str):\n",
    "        print(\"LLM output:\", output)\n",
    "    else:\n",
    "        print(\"Unknown output format:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_result(result) -> str:\n",
    "    output = result.get(\"output\", [])\n",
    "\n",
    "    print(f\"User query: {result.get('input')}\")\n",
    "    # if LLM returned a list of message blocks\n",
    "    if isinstance(output, list):\n",
    "        texts = []\n",
    "        for block in output:\n",
    "            if block.get(\"type\") == \"text\":\n",
    "                texts.append(block[\"text\"])\n",
    "        return \"\\n\".join(texts)\n",
    "\n",
    "    # if it's a plain string (some tools may return direct output)\n",
    "    elif isinstance(output, str):\n",
    "        return output\n",
    "\n",
    "    return \"Unrecognized response format.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    print(\"Legal Assistant (RAG). Type 'reset' to clear memory, 'exit' or 'quit' to leave the chat.\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in (\"exit\", \"quit\"):\n",
    "            break\n",
    "        elif user_input.lower() == \"reset\":\n",
    "            memory.clear()\n",
    "            print(\"Memory cleared.\\n\")\n",
    "            continue\n",
    "        try:\n",
    "            result = agent.invoke({\"input\": user_input})\n",
    "            print(f\"LLM output: {format_result(result)}\")\n",
    "            # ucomment below lines for debug\n",
    "            #print_result(result)\n",
    "            #print(f\"LLM output: {result['output']}\\n\")\n",
    "            #print(f\"test: {result['output']}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal Assistant (RAG). Type 'reset' to clear memory, 'exit' or 'quit' to leave the chat.\n",
      "\n",
      "User query: Please print offenses in 3015/09 in one short sentence\n",
      "LLM output: \n",
      "\n",
      "In case 3015/09, the defendant was convicted of rape, sodomy, kidnapping, robbery, and assault.\n",
      "User query: Find 3 similar case to case number 3015/09\n",
      "LLM output: \n",
      "\n",
      "Here are the three most similar cases to case 3015/09:\n",
      "1. Case 6068/21 (similarity score: 0.566)\n",
      "2. Case 9387/16 (similarity score: 0.447)\n",
      "3. Case 5956/04 (similarity score: 0.441)\n",
      "\n",
      "These cases likely share similar legal issues, facts, or reasoning with your reference case 3015/09. The similarity scores range from 0 to 1, where 1 indicates perfect similarity.\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_3_12_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
